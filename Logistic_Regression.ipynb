{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1:-What is Logistic Regression, and how does it differ from Linear Regression?**\n",
        "\n",
        "**Ans:-**Logistic Regression vs. Linear Regression\n",
        "\n",
        "**Linear Regression:**\n",
        "\n",
        "Purpose: It is used for predicting a continuous output (i.e., a regression problem). The output variable is a real-valued number.\n",
        "\n",
        "Model: The model assumes a linear relationship between the dependent variable\n",
        "𝑦 and the independent variable(s) 𝑋.\n",
        "\n",
        "**Logistic Regression:**\n",
        "\n",
        "Purpose: It is used for binary classification problems, i.e., predicting a categorical outcome with two possible outcomes, such as \"Yes\" or \"No\", \"Spam\" or \"Not Spam\", etc. The output variable is categorical, typically coded as 0 or 1.\n",
        "\n",
        "Model: The model assumes a linear relationship between the dependent variable\n",
        "𝑦 and the independent variables 𝑋. However, the output of the model is transformed using the sigmoid function to ensure it lies between 0 and 1, representing a probability.\n",
        "\n",
        "**Conclusion:**\n",
        "Use linear regression when you're working with continuous outcomes.\n",
        "\n",
        "Use logistic regression when you need to predict probabilities or classify into two categories (binary classification).\n",
        "\n",
        "**Q2:-What is the mathematical equation of Logistic Regression?**\n",
        "\n",
        "**Ans**\n",
        "The mathematical equation for Logistic Regression is based on the logistic function (sigmoid function). It is used to model the probability that a given input point belongs to a particular class.\n",
        "\n",
        "**Logistic Regression Equation:**\n",
        "\n",
        "The equation of the logistic regression model is:\n",
        "\n",
        "𝑃(𝑦=1∣𝑋) =1/1+𝑒−(𝛽0+𝛽1𝑥1+𝛽2𝑥2+⋯+𝛽𝑛𝑥𝑛)\n",
        "\n",
        "**Where:**\n",
        "\n",
        "=>𝑃(𝑦=1∣𝑋) is the probability of the target variable y being 1, given the input features X.\n",
        "\n",
        "=>𝛽0is the intercept (bias).\n",
        "\n",
        "=>𝛽1,𝛽2,…,𝛽𝑛 are the model's coefficients for the input features\n",
        "𝑥1,𝑥2,…,𝑥𝑛.\n",
        "\n",
        "=>e is the base of the natural logarithm.\n",
        "\n",
        "The function P(y=1∣X) outputs a value between 0 and 1, representing the probability that the instance belongs to class 1.\n",
        "\n",
        "**Q3:-Why do we use the Sigmoid function in Logistic Regression?**\n",
        "\n",
        "**Ans:-**\n",
        "The Sigmoid function is used in Logistic Regression because it helps to map the output of a linear function (which can range from −∞ to +∞) into a probability value between 0 and 1. This transformation is crucial for classification tasks, especially binary classification, where the goal is to predict the probability of an instance belonging to one of two classes.\n",
        "\n",
        "**Reason why is the Sigmoid function used in Logistic Regression**\n",
        "\n",
        "**1. Probability Output Between 0 and 1:**\n",
        "\n",
        "Logistic Regression is used for binary classification tasks, where the output is either 0 (negative class) or 1 (positive class). Probabilities must always lie between 0 and 1, as they represent the likelihood of an event occurring.\n",
        "\n",
        "The Sigmoid function squashes any real-valued number (from −∞ to +∞) into the range (0, 1), which allows us to interpret the model's output as a probability:\n",
        "\n",
        "𝑃(𝑦=1∣𝑋)=1/1+𝑒−𝑧\n",
        "\n",
        "**Where:**\n",
        "\n",
        "𝑧=𝛽0+𝛽1𝑥1+𝛽2𝑥2+⋯+𝛽𝑛𝑥𝑛is the linear combination of the features.\n",
        "\n",
        "e is the base of the natural logarithm.\n",
        "\n",
        "This transformation is what makes Logistic Regression a suitable model for classification. For example, if the sigmoid function outputs a probability of 0.8, we interpret this as \"there is an 80% chance that the instance belongs to class 1.\"\n",
        "\n",
        "**2. Non-linearity:**\n",
        "\n",
        "Without the Sigmoid function, Logistic Regression would simply be a linear regression model, which predicts continuous values. In a classification setting, this wouldn't work well because we need to classify observations into distinct categories (e.g., 0 or 1).\n",
        "\n",
        "The Sigmoid function introduces non-linearity, turning the output of the linear equation into a non-linear probability, allowing us to make decisions about class membership based on a threshold (e.g., if the probability is greater than or equal to 0.5, classify as 1; otherwise, classify as 0).\n",
        "\n",
        "**3. Log-Odds and Logistic Function:**\n",
        "\n",
        "In the context of Logistic Regression, the linear combination of the features (\n",
        "𝑧=𝛽0+𝛽1𝑥1+𝛽2𝑥2+…) is modeled as a log-odds. The log-odds represent the logarithm of the odds that an instance belongs to class 1, compared to class 0.\n",
        "\n",
        "The Sigmoid function maps these log-odds to a probability:\n",
        "P(y=1∣X)=σ(z)= 1/1+e^−z\n",
        "\n",
        "**Where**\n",
        "\n",
        "𝑧is the log-odds of class 1. This is why the model is called Logistic Regression — it models the log-odds of the outcome.\n",
        "\n",
        "In short, the Sigmoid function is crucial in Logistic Regression because it allows us to model probabilities and classify data into two classes in a meaningful way.\n",
        "\n",
        "**Q4:-What is the cost function of Logistic Regression?**\n",
        "\n",
        "**Ans:-**\n",
        "In Logistic Regression, the cost function (also known as the loss function) is used to measure how well the model’s predictions match the actual values. The goal is to minimize this cost function during training, which allows the model to learn the best-fitting parameters (coefficients) for the input data.\n",
        "\n",
        "Cost Function for Logistic Regression:\n",
        "The cost function used in logistic regression is called the Logistic Loss or Binary Cross-Entropy Loss. It quantifies the difference between the predicted probabilities and the true labels. The formula for the cost function is:\n",
        "\n",
        "J(θ)= −1/m ∑ to the power i=1[yto the power(i)log(hθ(xto the power(i))) + (1−yto the power(i))log(1−hθ(xto the power(i)))]\n",
        "\n",
        "Where:\n",
        "\n",
        "Jto the power(θ) is the cost function, which we want to minimize.\n",
        "\n",
        "m is the total number of training examples.\n",
        "\n",
        "xto the power(i)is the feature vector of the i-th training example.\n",
        "\n",
        "yto the power(i)is the actual label (0 or 1) of the i-th training example.\n",
        "\n",
        "hθ​(xto the power(i))= 1/1+eto the power−θTxto the power(i)1is the predicted probability that the i-th instance belongs to class 1. This is the sigmoid function applied to the linear combination of the input features 𝑥(𝑖)x (i)and parameters 𝜃.\n",
        "\n",
        "**Explanation of the Formula:**\n",
        "\n",
        "**Logarithmic Terms:**\n",
        "\n",
        "If the true label yto the power(i)=1, the term yto the power(i) log(h θ(xto the power(i) )) becomes significant, and we’re penalizing the model for incorrectly predicting class 0.\n",
        "\n",
        "If the true label 𝑦to the power(𝑖)=0, the term (1−yto the power(i) )log(1−hθ(xto the power(i))) becomes significant, and we’re penalizing the model for incorrectly predicting class 1.\n",
        "\n",
        "**Averaging Over All Training Examples:** The cost function is averaged over all the training examples (hence the division by 𝑚) to give a measure of the average loss across the entire dataset.\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "The cost function is 0 if the model makes perfect predictions for all examples.\n",
        "The higher the cost, the worse the model's predictions are.\n",
        "The goal is to minimize the cost function, adjusting the model’s parameters (\n",
        "θ) during training.\n",
        "\n",
        "**Why the Log-Loss (Binary Cross-Entropy)?**\n",
        "\n",
        "**Binary classification:** Logistic Regression is typically used for binary classification tasks, where the target variable\n",
        "𝑦has two classes: 0 or 1.\n",
        "\n",
        "**Logarithmic form:** The logarithm in the cost function ensures that large mistakes (i.e., predicting a value far from the true label) incur a larger penalty, while smaller mistakes are penalized less.\n",
        "\n",
        "**Probability interpretation:** Since the output of logistic regression is a probability, the cost function measures how \"surprising\" the prediction is, given the actual label.\n",
        "\n",
        "**Gradient Descent Optimization:**\n",
        "\n",
        "To minimize the cost function, we typically use Gradient Descent. This method updates the parameters 𝜃 iteratively to reduce the cost function.\n",
        "\n",
        "The gradient of the cost function with respect to 𝜃𝑗(the parameters) is given by:\n",
        "\n",
        "∂𝐽(𝜃)/∂𝜃𝑗=1/𝑚∑^𝑚 𝑖=1(ℎ𝜃(𝑥to the power(𝑖))−𝑦to the power(𝑖))𝑥𝑗to the power(𝑖)\n",
        "\n",
        "This tells us the direction in which the cost function is increasing or decreasing, and we update 𝜃𝑗θ jin the opposite direction (to minimize the cost function).\n",
        "\n",
        "**Q5:-What is Regularization in Logistic Regression? Why is it needed?**\n",
        "\n",
        "**Ans:-**\n",
        "**Regularization in Logistic Regression:**\n",
        "\n",
        "Regularization is a technique used in Logistic Regression (and other machine learning models) to prevent overfitting by adding a penalty to the model’s complexity. It does this by discouraging the model from fitting the training data too closely, which helps it generalize better to new, unseen data.\n",
        "\n",
        "In the context of Logistic Regression, regularization is used to penalize large coefficients (weights), forcing the model to learn simpler, more generalizable patterns.\n",
        "\n",
        "**Why Regularization is Needed:**\n",
        "\n",
        "**Overfitting:**\n",
        "\n",
        "Overfitting occurs when a model learns not only the true underlying patterns in the data but also the noise or random fluctuations. This results in a model that performs very well on the training data but poorly on new, unseen data.\n",
        "Regularization helps reduce overfitting by preventing the model from learning overly complex patterns that do not generalize well.\n",
        "\n",
        "**Improving Generalization:**\n",
        "\n",
        "Regularization helps the model generalize better to unseen data by adding a penalty for large coefficients, which encourages the model to make simpler decisions.\n",
        "\n",
        "**Avoiding Large Weights:**\n",
        "\n",
        "When the model’s weights become too large, the model may become too sensitive to small fluctuations in the data, causing it to overfit. Regularization controls the size of the weights to prevent this.\n",
        "\n",
        "**Q6:-Explain the difference between Lasso, Ridge, and Elastic Net regression?**\n",
        "\n",
        "**Ans:-**\n",
        "Lasso, Ridge, and Elastic Net Regression:\n",
        "These three are popular regularization techniques used in regression models to prevent overfitting and improve generalization. Each one applies a different penalty to the model's coefficients (weights), which has distinct effects on the learned model.\n",
        "\n",
        "**1. Ridge Regression (L2 Regularization):**\n",
        "\n",
        "**Penalty:** Ridge regression applies L2 regularization, which penalizes the sum of the squared values of the coefficients.\n",
        "\n",
        "**Cost Function:**\n",
        "\n",
        "J(θ)= 1/2m i=1∑m (y to the power (i) − yto the power(i) ) to the power 2 +λ j=1∑n θ j2\n",
        "\n",
        "**Effect on Coefficients:** Ridge regularization reduces the magnitude of coefficients but does not set them exactly to zero. It shrinks all coefficients equally toward zero, but they are still likely to remain non-zero.\n",
        "\n",
        "**Use Case:** Ridge regression is suitable when you believe all features have some effect on the outcome but you want to prevent the model from becoming too complex.\n",
        "\n",
        "**2. Lasso Regression (L1 Regularization):**\n",
        "\n",
        "**Penalty:** Lasso regression applies L1 regularization, which penalizes the sum of the absolute values of the coefficients.\n",
        "\n",
        "**Cost Function:**\n",
        "\n",
        "J(θ)= 2m/1i=1∑m(yto the power(i) − y^to the power(i) ) to the power 2 +λ j=1∑n ∣θ j∣\n",
        "\n",
        "**Effect on Coefficients:** Lasso regularization can shrink some coefficients to exactly zero, effectively performing feature selection by removing irrelevant or redundant features.\n",
        "\n",
        "**Use Case:** Lasso regression is useful when you expect only a few features to be important and want to automatically perform feature selection by driving some coefficients to zero.\n",
        "\n",
        "**3. Elastic Net Regression:**\n",
        "\n",
        "**Penalty:** Elastic Net is a combination of L1 (Lasso) and L2 (Ridge) regularization. It applies both the sum of absolute values and the sum of squared values of the coefficients.\n",
        "\n",
        "Cost Function: J(θ)= 1/2m i=1∑^m (y to the power (i) − y^to the power(i))to the power2 +λ 1j=1∑n ∣θ j ∣+λ 2j=1∑n​ θ j2\n",
        "\n",
        "**Effect on Coefficients:** Elastic Net encourages both sparsity (like Lasso) and shrinkage (like Ridge). It can help in cases where there are many correlated features, and it benefits from the strengths of both L1 and L2 regularization.\n",
        "\n",
        "**Use Case:** Elastic Net is suitable when there are many correlated features, and you want to combine the advantages of both Lasso and Ridge regularization. It’s often the go-to method when Lasso or Ridge alone doesn’t perform well.\n",
        "\n",
        "**Q7:-When should we use Elastic Net instead of Lasso or Ridge?**\n",
        "\n",
        "**Ans:-\n",
        "Elastic Net is a linear regression technique that combines both Lasso and Ridge regularization. It’s particularly useful when you have a large number of correlated features, as it can handle the situations where Lasso (which uses L1 regularization) may struggle due to its tendency to select only one feature from a group of highly correlated features.\n",
        "\n",
        "**Here's when you should use Elastic Net over Lasso or Ridge:**\n",
        "\n",
        "**When you have highly correlated features:**\n",
        "\n",
        "Lasso (L1 regularization) tends to select only one feature from a group of highly correlated features and shrink the rest to zero. If you have a set of correlated features and don't want to lose them all, Elastic Net can help by keeping them together (through the group effect of the L2 penalty).\n",
        "\n",
        "Ridge (L2 regularization) tends to keep all correlated features in the model but assigns them smaller coefficients. However, it doesn't perform variable selection.\n",
        "\n",
        "Elastic Net works by combining both L1 and L2 penalties, making it a good choice when you want the benefits of both Lasso and Ridge.\n",
        "\n",
        "**When you want the benefits of both Lasso and Ridge:**\n",
        "\n",
        "Lasso is great when you want to perform feature selection, and Ridge is better when you want to keep all features but shrink their coefficients. Elastic Net provides a middle ground and can benefit from both.\n",
        "\n",
        "**When there are more features than samples:**\n",
        "\n",
        "Elastic Net performs better than Lasso in situations where the number of features is large, and there might be multicollinearity. Since it combines L1 and L2 penalties, it can perform feature selection while maintaining the stability of Ridge.\n",
        "\n",
        "**When tuning L1 vs L2 penalties is required:**\n",
        "\n",
        "Elastic Net allows you to control the mix between Lasso and Ridge regularization through the parameter α. This is useful when you want to explore which regularization technique (Lasso or Ridge) performs better on your data.\n",
        "\n",
        "**Q8:-What is the impact of the regularization parameter (λ) in Logistic Regression?**\n",
        "\n",
        "**Ans:-**\n",
        "In Logistic Regression, the regularization parameter (often denoted as λ or alpha) controls the strength of the penalty applied to the model's coefficients. Regularization is used to prevent overfitting by penalizing large coefficient values, which might otherwise lead to a model that fits the training data too closely, capturing noise rather than the underlying patterns.\n",
        "\n",
        "In Logistic Regression, regularization works through either L1 regularization (Lasso) or L2 regularization (Ridge), or a combination of both (ElasticNet). The impact of the regularization parameter λ (or alpha) is crucial because it determines the amount of regularization applied to the model.\n",
        "\n",
        "**Key Impacts of Regularization Parameter (λ) on Logistic Regression:**\n",
        "\n",
        "**Preventing Overfitting:**\n",
        "\n",
        "A small λ (or alpha) means less regularization, so the model can fit the training data more closely. This increases the risk of overfitting, where the model becomes too complex and performs poorly on unseen data.\n",
        "\n",
        "A large λ (or alpha) imposes a higher penalty on the coefficients, leading to simpler models with smaller coefficients. This reduces overfitting but might lead to underfitting if λ is too large, making the model too simple to capture the underlying patterns in the data.\n",
        "\n",
        "**Shrinking Coefficients:**\n",
        "\n",
        "For L2 regularization (Ridge), the regularization parameter λ controls the magnitude of the penalty applied to the squared coefficients. It shrinks all coefficients but does not set them exactly to zero. The larger the λ, the more the coefficients are penalized.\n",
        "\n",
        "For L1 regularization (Lasso), λ controls the penalty on the absolute value of the coefficients. Lasso can shrink some coefficients to exactly zero, effectively performing feature selection. Increasing λ in Lasso increases the number of zeroed coefficients.\n",
        "\n",
        "**Model Complexity:**\n",
        "\n",
        "λ directly impacts the model complexity. A higher λ leads to simpler models with fewer or smaller coefficients, while a smaller λ leads to more complex models with potentially larger coefficients.\n",
        "\n",
        "In cases where ElasticNet regularization is used (a mix of L1 and L2), λ controls the total penalty, and an additional parameter l1_ratio controls the mix of L1 and L2 penalties.\n",
        "\n",
        "**Q9:-What are the key assumptions of Logistic Regression?\n",
        "\n",
        "**Ans:-**\n",
        "Logistic Regression is a powerful and widely used classification algorithm in machine learning, but it makes several key assumptions about the data. These assumptions are important for ensuring that the model performs well and that the results are interpretable. If these assumptions are violated, the model’s predictions may be biased or inaccurate.\n",
        "\n",
        "Here are the key assumptions of Logistic Regression:\n",
        "\n",
        "1. Linearity of the Logit (Log-Odds):\n",
        "Logistic Regression assumes that there is a linear relationship between the independent variables (predictors) and the log-odds of the dependent variable.\n",
        "\n",
        "Specifically, it assumes that the log-odds of the probability of the target being in a certain class is a linear combination of the features. This means that:\n",
        "\n",
        "log( p/p-1)=β0 +β1X1+β2X2+⋯+βnXn\n",
        "\n",
        "where\n",
        "\n",
        "p is the probability of the positive class, and X 1 ,X 2 ,…,X nare the independent variables.\n",
        "\n",
        "What this means in practice: Even if the relationship between the features and the target variable is nonlinear, as long as the relationship between the predictors and the log-odds is linear, the model can still perform well. If this assumption is violated, other models like Decision Trees or Random Forests (which can capture nonlinear relationships) might be better choices.\n",
        "\n",
        "**2. Independence of Observations:**\n",
        "\n",
        "Logistic Regression assumes that the observations (data points) are independent of each other. This means that the outcome for one observation should not depend on or be influenced by the outcome of another observation.\n",
        "\n",
        "What this means in practice: This assumption is critical for many real-world datasets. If there’s a correlation between observations (such as time-series data or clustered data), you might need to use methods like Generalized Estimating Equations (GEE) or Mixed-Effects Models to account for this dependence.\n",
        "\n",
        "**3. No or Little Multicollinearity:**\n",
        "\n",
        "Logistic Regression assumes that the independent variables are not highly correlated with each other. High correlations between features can cause issues like multicollinearity, which makes the model's coefficients unstable and hard to interpret.\n",
        "\n",
        "What this means in practice: You should check for multicollinearity before training a logistic regression model. One common way to check for multicollinearity is by calculating the Variance Inflation Factor (VIF) for each feature. If the VIF is large (typically above 10), multicollinearity might be an issue.\n",
        "\n",
        "**4. Large Sample Size:**\n",
        "\n",
        "Logistic Regression generally assumes that there is a sufficiently large sample size. With a small sample size, the model might not be reliable and may produce biased estimates.\n",
        "\n",
        "What this means in practice: Logistic Regression estimates the probabilities based on the maximum likelihood estimation, which is most effective with large datasets. A small sample size could lead to overfitting or underfitting and make it hard to obtain stable parameter estimates.\n",
        "\n",
        "**5. No Extreme Outliers:**\n",
        "\n",
        "Logistic Regression is sensitive to outliers because outliers can disproportionately influence the model’s coefficients. Extreme values in the data can make the model overly complex, resulting in poor generalization.\n",
        "\n",
        "What this means in practice: You should check for and handle outliers before training the model. This can be done using visualization techniques like boxplots or statistical methods like Z-scores to identify outliers.\n",
        "\n",
        "**6. Binary Outcome Variable (for Binary Logistic Regression):**\n",
        "\n",
        "For binary logistic regression, the target variable is assumed to be binary (i.e., it takes two possible values, such as 0 and 1, Yes and No, True and False).\n",
        "\n",
        "What this means in practice: Logistic regression is designed to predict probabilities for two classes. If you have more than two categories, you would typically use Multinomial Logistic Regression or Ordinal Logistic Regression instead.\n",
        "\n",
        "**7. Homoscedasticity (for Continuous Features):**\n",
        "\n",
        "While this assumption is more important for linear regression, logistic regression benefits from having homoscedasticity in the predictors. In other words, the variance of errors is constant across all levels of the predictors.\n",
        "\n",
        "What this means in practice: If your model exhibits heteroscedasticity (i.e., the error variance increases or decreases with the level of predictors), you might consider transformations or alternative models.\n",
        "\n",
        "**Q10:-What are some alternatives to Logistic Regression for classification tasks?**\n",
        "\n",
        "**Ans:-**\n",
        "There are several alternatives to Logistic Regression for classification tasks, each with its own strengths and weaknesses. Below are some of the most popular alternatives, along with brief descriptions and examples of how to use them in Python.\n",
        "\n",
        "**1. K-Nearest Neighbors (KNN)**\n",
        "\n",
        "Description: K-Nearest Neighbors is a non-parametric, instance-based learning algorithm. It classifies a data point based on the majority class of its nearest neighbors in the feature space. It doesn't assume anything about the underlying distribution of the data.\n",
        "\n",
        "When to use: KNN works well when the decision boundary is non-linear and is ideal for smaller datasets. It can struggle with high-dimensional data due to the curse of dimensionality.\n",
        "\n",
        "**2. Decision Trees**\n",
        "\n",
        "Description: Decision Trees partition the feature space into regions and classify data based on these regions. They create a flowchart-like structure where each internal node represents a decision based on a feature, and each leaf node represents a class label.\n",
        "\n",
        "When to use: Decision Trees work well with both categorical and continuous data and are interpretable. However, they are prone to overfitting, especially with deep trees.\n",
        "\n",
        "**3. Random Forest**\n",
        "\n",
        "Description: Random Forest is an ensemble method that combines multiple decision trees, typically trained with bootstrapped data (subsamples of the dataset). It reduces overfitting by averaging the results of many trees and is robust to noise.\n",
        "\n",
        "When to use: Random Forest works well for high-dimensional data and can handle both classification and regression tasks. It also performs well on datasets with complex relationships.\n",
        "\n",
        "**4. Support Vector Machine (SVM)**\n",
        "\n",
        "Description: SVM constructs a hyperplane or set of hyperplanes in a high-dimensional space that can be used for classification. It aims to find the hyperplane that best separates the classes by maximizing the margin between the classes.\n",
        "\n",
        "When to use: SVM is effective in high-dimensional spaces and is good for non-linear decision boundaries. It's particularly useful when the number of features is large compared to the number of samples.\n",
        "\n",
        "**5. Naive Bayes**\n",
        "\n",
        "Description: Naive Bayes is based on Bayes' Theorem and assumes that features are conditionally independent given the class label. There are different types of Naive Bayes classifiers (Gaussian, Multinomial, Bernoulli), depending on the nature of the data.\n",
        "\n",
        "When to use: Naive Bayes works well when the features are conditionally independent, and is particularly effective for text classification (e.g., spam detection).\n",
        "\n",
        "**6. Gradient Boosting Machines (GBM)**\n",
        "\n",
        "Description: GBM is an ensemble learning method that builds models in a sequential manner. Each new model corrects the errors made by the previous model. The most popular variant of GBM is XGBoost, which is optimized for speed and performance.\n",
        "\n",
        "When to use: GBMs are powerful, often outperforming other models on structured/tabular data. They are effective for both classification and regression tasks, but can be prone to overfitting if not carefully tuned.\n",
        "\n",
        "**7. Neural Networks (Multilayer Perceptrons)**\n",
        "\n",
        "Description: A Neural Network is a model inspired by the way the human brain works. It consists of layers of nodes (neurons) where each node performs a simple mathematical operation, and the output is passed to the next layer. The most common type is the Multilayer Perceptron (MLP).\n",
        "\n",
        "When to use: Neural Networks are powerful for handling complex, non-linear relationships in the data. They are particularly useful for large datasets and high-dimensional problems like image or speech recognition.\n",
        "\n",
        "**Q11:-What are Classification Evaluation Metrics?**\n",
        "\n",
        "**Ans:-**\n",
        "Classification Evaluation Metrics are used to assess how well a classification model is performing. These metrics help you understand the quality of your model by evaluating different aspects of its performance, such as how accurately it classifies data, how it handles imbalanced classes, and how well it distinguishes between the different classes.\n",
        "\n",
        "Here’s a list of commonly used classification evaluation metrics in Python, along with examples of how to calculate and use them.\n",
        "\n",
        "**1. Accuracy**\n",
        "\n",
        "Definition: The proportion of correctly predicted samples in the dataset.\n",
        "\n",
        "Formula:Accuracy=TP+TN/TP+FP+TN+FN\n",
        "\n",
        "Use Case: It is useful when the classes are balanced.\n",
        "\n",
        "**2. Precision**\n",
        "\n",
        "Definition: The ratio of correctly predicted positive observations to the total predicted positives.\n",
        "\n",
        "Formula:Precision=TP/TP+FP\n",
        "\n",
        "Use Case: Important when the cost of false positives is high (e.g., spam detection, fraud detection).\n",
        "\n",
        "**3. Recall (Sensitivity or True Positive Rate)**\n",
        "\n",
        "Definition: The ratio of correctly predicted positive observations to all actual positives.\n",
        "\n",
        "Formula:Recall=TP/TP+FN\n",
        "\n",
        "Use Case: Important when the cost of false negatives is high (e.g., medical diagnosis).\n",
        "\n",
        "**4. F1-Score**\n",
        "\n",
        "Definition: The harmonic mean of Precision and Recall. It balances the trade-off between Precision and Recall.\n",
        "\n",
        "Formula:F1-Score=2×Precision×Recall/Precision+Recall\n",
        "\n",
        "Use Case: It is useful when you need a balance between Precision and Recall, especially in imbalanced datasets.\n",
        "\n",
        "**5. Confusion Matrix**\n",
        "\n",
        "Definition: A table that shows the performance of the classification model by comparing the actual and predicted labels. It provides insights into True Positives, False Positives, True Negatives, and False Negatives.\n",
        "\n",
        "**6. ROC Curve and AUC (Area Under the Curve)**\n",
        "\n",
        "Definition: The ROC curve is a graphical representation of the classifier's ability to distinguish between classes at all possible thresholds. The AUC represents the area under this curve, giving a single value to summarize the model's ability to separate the classes.\n",
        "\n",
        "Use Case: Useful for binary classification tasks. AUC ranges from 0 to 1, with 1 representing perfect classification.\n",
        "\n",
        "**7. Precision-Recall Curve**\n",
        "\n",
        "Definition: A curve that plots Precision against Recall for different thresholds.\n",
        "\n",
        "Use Case: More informative than ROC when dealing with imbalanced datasets, especially for binary classification.\n",
        "\n",
        "**8. Matthews Correlation Coefficient (MCC)**\n",
        "\n",
        "Definition: A metric that combines True Positives, False Positives, True Negatives, and False Negatives to produce a score between -1 and 1. An MCC of 1 means a perfect prediction, 0 indicates random guessing, and -1 means completely incorrect predictions.\n",
        "\n",
        "**9. Log Loss (Cross-Entropy Loss)**\n",
        "\n",
        "Definition: Log loss measures the performance of a classification model where the output is a probability value between 0 and 1. The model is penalized based on how far its predicted probabilities are from the actual class.\n",
        "\n",
        "**10. Specificity (True Negative Rate)**\n",
        "\n",
        "Definition: The ratio of correctly predicted negative observations to all actual negative observations.\n",
        "\n",
        "Formula:Specificity=TN/TN+FP\n",
        "\n",
        "Use Case: Important when you want to assess how well your model is identifying negative cases.\n",
        "\n",
        "**Q12:-How does class imbalance affect Logistic Regression?**\n",
        "\n",
        "**Ans:-**\n",
        "Class imbalance occurs when the classes in a dataset are not represented equally, meaning one class has significantly more samples than the other(s). In the context of Logistic Regression (or any classification algorithm), class imbalance can have several effects on model performance. Here's a breakdown of how it can affect Logistic Regression and how you can address these issues in Python.\n",
        "\n",
        "**Impact of Class Imbalance on Logistic Regression**\n",
        "\n",
        "**Bias Toward the Majority Class:**\n",
        "\n",
        "Logistic Regression, like many machine learning algorithms, tends to be biased toward the majority class when the dataset is imbalanced. This is because the model focuses on minimizing the overall error, and since the majority class is dominant, the model may predict the majority class more often, even when it's incorrect.\n",
        "\n",
        "This leads to high accuracy but poor performance in predicting the minority class.\n",
        "\n",
        "**Misleading Evaluation Metrics:**\n",
        "\n",
        "Accuracy becomes a misleading metric in imbalanced datasets. For instance, if 90% of your data belongs to class A (majority class) and 10% belongs to class B (minority class), a model that predicts only class A will still have 90% accuracy. However, it fails to identify the minority class, which is often the class of more interest.\n",
        "Precision, Recall, and F1-score become more relevant, as they account for both the true positives and the false positives for the minority class.\n",
        "\n",
        "**Poor Generalization to Minority Class:**\n",
        "\n",
        "Logistic Regression may have a harder time learning the characteristics of the minority class due to its underrepresentation in the dataset. This could lead to low recall or high false negative rates for the minority class.\n",
        "\n",
        "**Q13:-What is Hyperparameter Tuning in Logistic Regression?**\n",
        "\n",
        "**Ans:-**\n",
        "Hyperparameter tuning in logistic regression (or any machine learning model) refers to the process of selecting the best set of hyperparameters that optimize the model's performance. In the case of logistic regression, hyperparameters are the settings that are not learned from the training data, but are set before training begins.\n",
        "\n",
        "Here are the common hyperparameters involved in tuning a logistic regression model:\n",
        "\n",
        "**Regularization (C):**\n",
        "\n",
        "C is the inverse of regularization strength. It controls the tradeoff between fitting the model well to the training data and keeping the model simple (i.e., avoiding overfitting). A small value of C applies strong regularization (simpler model), while a larger value allows the model to fit the data more closely (less regularization).\n",
        "Typical values: 0.001, 0.01, 0.1, 1, 10, 100, 1000\n",
        "\n",
        "**Penalty:**\n",
        "\n",
        "The penalty used for regularization can be:\n",
        "l1: Lasso regularization (encourages sparsity).\n",
        "l2: Ridge regularization (encourages small coefficients).\n",
        "Common choice: l2 is used by default, but l1 can be useful for feature selection.\n",
        "\n",
        "**Solver:**\n",
        "\n",
        "The algorithm used to find the best parameters during training.\n",
        "liblinear: A good choice for small datasets and when using l1 penalty.\n",
        "newton-cg, lbfgs, sag, saga: Suitable for large datasets, especially for l2 regularization.\n",
        "\n",
        "**Max Iterations:**\n",
        "\n",
        "The maximum number of iterations the solver will perform. Increasing this number can sometimes help when the model is not converging.\n",
        "Typical value: 100, 200, 300 (can be increased if needed).\n",
        "\n",
        "**Q14:-What are different solvers in Logistic Regression? Which one should be used?**\n",
        "\n",
        "**Ans:-**\n",
        "In Logistic Regression, solvers are the algorithms used to optimize the model parameters during training. Each solver uses a different approach to minimize the cost function (usually the log-likelihood function in logistic regression) and converge to the best parameter values. The choice of solver can affect the speed and accuracy of the model training, depending on the dataset and problem you're working on.\n",
        "\n",
        "Here are the main solvers available in sklearn.linear_model.LogisticRegression:\n",
        "\n",
        "**1. liblinear**\n",
        "\n",
        "Type: A solver based on coordinate descent for small datasets.\n",
        "\n",
        "Usage: This is the default solver when the penalty is l1 (Lasso regularization) and can also work with l2 penalty.\n",
        "\n",
        "Speed: Best suited for small to medium-sized datasets.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Good for small datasets.\n",
        "\n",
        "Works well with both l1 and l2 regularization.\n",
        "\n",
        "Disadvantages: Can be slow with large datasets.\n",
        "\n",
        "Use when: You are using L1 regularization (penalty='l1'), or working with smaller datasets.\n",
        "\n",
        "**2. newton-cg**\n",
        "\n",
        "Type: Uses Newton’s method, a second-order optimization algorithm.\n",
        "\n",
        "Usage: Suitable for l2 regularization. It's efficient and works well for large datasets.\n",
        "\n",
        "Speed: Works better for large datasets compared to liblinear.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Efficient for large datasets.\n",
        "\n",
        "Can handle l2 penalty effectively.\n",
        "\n",
        "Disadvantages: Doesn’t support l1 penalty.\n",
        "\n",
        "Use when: You are working with l2 regularization and have a larger dataset.\n",
        "\n",
        "**3. lbfgs**\n",
        "\n",
        "Type: Uses Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) algorithm, a variant of Newton's method.\n",
        "\n",
        "Usage: Suitable for l2 regularization and works well with large datasets.\n",
        "\n",
        "Speed: Similar to newton-cg and often works faster in practice for large datasets.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Efficient for large datasets.\n",
        "\n",
        "Handles l2 regularization well.\n",
        "\n",
        "Disadvantages: Doesn’t support l1 penalty.\n",
        "\n",
        "Use when: You are working with l2 regularization and have a larger dataset.\n",
        "\n",
        "**4. sag**\n",
        "\n",
        "Type: Stochastic Average Gradient (SAG) is a variant of stochastic gradient descent that uses a more efficient way of updating weights.\n",
        "\n",
        "Usage: Works best with l2 regularization and is designed for very large datasets.\n",
        "\n",
        "Speed: Efficient for large datasets, especially those with many features.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Scalable to very large datasets.\n",
        "\n",
        "Works well with l2 regularization.\n",
        "\n",
        "Disadvantages: Can be slower for small datasets.\n",
        "\n",
        "Use when: You have a large dataset and are using l2 regularization.\n",
        "\n",
        "**5. saga**\n",
        "\n",
        "Type: Similar to sag, but it supports both l1 and l2 regularization.\n",
        "\n",
        "Usage: Supports l1 and l2 penalties, so it's very flexible.\n",
        "\n",
        "Speed: Efficient for large datasets and supports l1 regularization, which makes it more versatile than sag.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Works with both l1 and l2 regularization.\n",
        "\n",
        "Efficient for large datasets.\n",
        "\n",
        "Can handle non-smooth penalties like l1.\n",
        "\n",
        "Disadvantages: May be slower than liblinear for small datasets.\n",
        "\n",
        "Use when: You need l1 or l2 regularization and have a large dataset.\n",
        "\n",
        "**Q15:-How is Logistic Regression extended for multiclass classification?**\n",
        "\n",
        "**Ans:-**\n",
        "Logistic Regression is originally designed for binary classification (where the target variable has only two classes, 0 and 1). However, it can be extended to handle multiclass classification (where the target variable has more than two classes) using different strategies.\n",
        "\n",
        "The two primary approaches to extend logistic regression to multiclass classification are:\n",
        "\n",
        "**1. One-vs-Rest (OvR) or One-vs-All (OvA)**\n",
        "\n",
        "Concept: In the One-vs-Rest strategy, a separate binary logistic regression model is trained for each class. Each model predicts whether a data point belongs to a particular class or not, treating all other classes as the \"rest.\"\n",
        "\n",
        "How it works: If there are\n",
        "N classes, you train\n",
        "N binary classifiers, where each classifier is trained to distinguish one class from the rest.\n",
        "\n",
        "For example, for 3 classes (A, B, and C), we will have:\n",
        "Classifier 1: Predicts class A vs. (B and C)\n",
        "Classifier 2: Predicts class B vs. (A and C)\n",
        "Classifier 3: Predicts class C vs. (A and B)\n",
        "Prediction: After training, when making a prediction, the model that produces the highest probability is selected.\n",
        "\n",
        "**2. Softmax or Multinomial Logistic Regression**\n",
        "\n",
        "Concept: The multinomial logistic regression (or softmax regression) generalizes logistic regression to handle multiple classes directly. Instead of creating binary classifiers for each class, the model computes probabilities for all classes simultaneously and selects the class with the highest probability.\n",
        "\n",
        "How it works: It computes the probability of each class using the softmax function, which is an extension of the logistic function to multiple classes.\n",
        "\n",
        "The softmax function outputs a probability distribution over the\n",
        "N classes.\n",
        "\n",
        "Mathematics: Given the logistic regression model's score\n",
        "zj=w j⊤​x+bj for class j, the softmax function computes the probability for class\n",
        "\n",
        "j as:P(y=j∣x)= ∑ k=1N exp(z k )/exp(z j)\n",
        "\n",
        "where\n",
        "N is the number of classes, and the denominator sums over all classes.\n",
        "Prediction: The class with the highest probability is selected.\n",
        "\n",
        "**Q16:-What are the advantages and disadvantages of Logistic Regression?**\n",
        "\n",
        "**Ans:-**\n",
        "**Advantages of Logistic Regression:**\n",
        "\n",
        "Simplicity and Easy to Implement: Logistic regression is easy to understand and implement. It doesn't require complex computations and can be easily trained and deployed in various environments.\n",
        "\n",
        "Interpretable Model: The model coefficients in logistic regression provide an easy interpretation of the effect of each feature on the prediction outcome. This is particularly useful in cases where understanding the influence of each variable is important.\n",
        "\n",
        "Efficiency: Logistic regression is computationally efficient, especially for binary classification problems. It is faster than more complex models such as neural networks, especially on smaller datasets.\n",
        "\n",
        "Probabilistic Output: Logistic regression outputs probabilities, which can be useful for ranking predictions and providing confidence scores, rather than just the hard classification result.\n",
        "\n",
        "Less Prone to Overfitting (with regularization): By using techniques like L1 or L2 regularization, logistic regression can avoid overfitting, especially when the number of features is large relative to the number of data points.\n",
        "\n",
        "Works Well with Linearly Separable Data: If the classes are linearly separable or close to linear, logistic regression performs well. It can be a very strong baseline model for classification problems.\n",
        "\n",
        "**Disadvantages of Logistic Regression:**\n",
        "\n",
        "Limited to Linear Relationships: Logistic regression assumes a linear relationship between the independent variables and the log-odds of the outcome. It struggles to capture complex patterns if the data is not linearly separable.\n",
        "\n",
        "Sensitive to Outliers: Logistic regression can be sensitive to outliers, as the estimated coefficients can be significantly influenced by extreme values in the data. Proper data preprocessing and outlier detection can mitigate this to some extent.\n",
        "\n",
        "Requires Feature Scaling: Logistic regression requires features to be scaled, especially when using regularization. Without scaling, the model's performance can be degraded.\n",
        "\n",
        "Performance Degrades with High Dimensionality: As the number of features increases, the logistic regression model might suffer, especially in cases where there is multicollinearity (high correlation between independent variables). It can also suffer from the curse of dimensionality.\n",
        "\n",
        "Not Suitable for Non-linear Data: Logistic regression isn't well-suited for non-linear decision boundaries. While it can be adapted using kernel tricks or by introducing interaction terms, it’s still less flexible than algorithms like decision trees or SVMs with non-linear kernels.\n",
        "\n",
        "Assumes Independence of Features: Logistic regression assumes that the features are independent of each other. If this assumption is violated, the performance might decrease.\n",
        "\n",
        "Binary Output (Standard Logistic Regression): Standard logistic regression is a binary classifier. While you can extend it to multi-class classification using strategies like One-vs-Rest (OvR) or One-vs-One (OvO), it’s not as naturally suited to multi-class problems as models like decision trees or random forests.\n",
        "\n",
        "**Q17:-What are some use cases of Logistic Regression?**\n",
        "\n",
        "**Ans:-**\n",
        "Logistic regression is widely used in various fields because of its simplicity, interpretability, and effectiveness. Below are some common use cases of logistic regression, along with a brief explanation of how it can be applied:\n",
        "\n",
        "**1. Binary Classification Problems:**\n",
        "Logistic regression is best known for binary classification, where the outcome variable has two possible values (e.g., 0 or 1, True or False).\n",
        "\n",
        "Use Cases:\n",
        "\n",
        "Email Spam Detection: Predicting whether an email is spam (1) or not spam (0) based on features like the sender's address, the content of the email, or the frequency of certain words.\n",
        "\n",
        "Customer Churn Prediction: Predicting whether a customer will leave a service (1) or remain (0) based on features like customer behavior, usage patterns, and account history.\n",
        "\n",
        "Disease Diagnosis: Predicting whether a patient has a particular disease (1) or not (0) based on medical test results and patient demographics.\n",
        "\n",
        "**2. Multiclass Classification (One-vs-Rest/One-vs-One):**\n",
        "Although logistic regression is originally a binary classifier, it can be extended to handle multiclass problems by applying techniques like One-vs-Rest (OvR) or One-vs-One (OvO). This is useful when the target variable has more than two categories.\n",
        "\n",
        "Use Cases:\n",
        "\n",
        "Handwritten Digit Recognition: Classifying digits (0-9) based on pixel features from an image.\n",
        "\n",
        "Category Prediction: Predicting the category of an article (e.g., politics, sports, technology, etc.) from a set of predefined categories.\n",
        "\n",
        "**3. Predicting Probability of an Event:**\n",
        "Logistic regression can predict the probability of an event occurring. This is one of the key benefits of the model because it provides a probabilistic output (values between 0 and 1).\n",
        "\n",
        "Use Cases:\n",
        "\n",
        "Loan Approval Prediction: Predicting the probability of a loan being approved based on applicant features (e.g., credit score, income, loan amount).\n",
        "\n",
        "Marketing Campaign Response: Predicting the likelihood of a customer responding to a marketing campaign, allowing businesses to target customers who are more likely to convert.\n",
        "\n",
        "**4. Credit Scoring and Fraud Detection:**\n",
        "Logistic regression is widely used in the finance industry to assess the likelihood of credit default or fraud detection. By analyzing the transaction history, user behavior, and financial features, logistic regression can help classify whether a transaction is fraudulent or not.\n",
        "\n",
        "Use Cases:\n",
        "\n",
        "Credit Scoring: Predicting the probability that an individual will default on a loan based on financial features like income, employment status, and credit history.\n",
        "\n",
        "Fraud Detection in Banking Transactions: Detecting whether a transaction is fraudulent or genuine based on user behavior and transaction patterns.\n",
        "\n",
        "**5. A/B Testing (Comparing Two Groups):**\n",
        "Logistic regression can be used in A/B testing experiments to assess the effect of two different versions of a product, website design, or advertisement.\n",
        "\n",
        "Use Cases:\n",
        "\n",
        "Website Optimization: Analyzing user interaction with different versions of a webpage to predict whether a user is more likely to click on a given call to action based on the page they saw.\n",
        "\n",
        "Advertising Effectiveness: Comparing two ads to determine which ad leads to a higher conversion rate (e.g., sales or sign-ups).\n",
        "\n",
        "**6. Customer Segmentation and Targeting:**\n",
        "Logistic regression can be used to segment customers based on behavior or characteristics. The model can help businesses understand which customers are more likely to respond to specific actions, such as a promotional offer.\n",
        "\n",
        "Use Cases:\n",
        "\n",
        "Market Segmentation: Classifying customers into different segments (e.g., high-value customers, medium-value customers, and low-value customers) based on purchasing behavior.\n",
        "\n",
        "Targeted Campaigns: Predicting which customers are more likely to respond to specific marketing strategies.\n",
        "\n",
        "**7. Political Campaigns:**\n",
        "Logistic regression can be used in political campaigns to predict voter behavior. By analyzing voter demographics, past voting history, and other factors, it is possible to predict the likelihood that a person will vote for a certain candidate.\n",
        "\n",
        "Use Cases:\n",
        "\n",
        "Voter Turnout Prediction: Predicting whether an individual will vote or not in an election based on demographics, past voting behavior, and other influencing factors.\n",
        "\n",
        "Election Outcome Prediction: Predicting the probability of a candidate winning based on various survey data and campaign strategies.\n",
        "\n",
        "**8. Health Risk Prediction:**\n",
        "Logistic regression can be applied in healthcare to predict various health risks or conditions, providing valuable insights into how different factors influence the likelihood of a health event occurring.\n",
        "\n",
        "Use Cases:\n",
        "\n",
        "Heart Disease Prediction: Predicting the likelihood of a patient having heart disease based on factors like age, cholesterol levels, blood pressure, and family history.\n",
        "\n",
        "Diabetes Prediction: Predicting the likelihood of a person developing diabetes based on features such as BMI, age, lifestyle, and genetic factors.\n",
        "\n",
        "**Q18:-What is the difference between Softmax Regression and Logistic Regression?**\n",
        "\n",
        "**Ans:-**\n",
        "Softmax Regression and Logistic Regression are both classification algorithms, but they differ in the types of problems they are suited for and the way they handle the output. Below is a detailed explanation of the key differences:\n",
        "\n",
        "**1. Type of Classification Problem:**\n",
        "\n",
        "**Logistic Regression (Binary Classification):**\n",
        "\n",
        "Logistic regression is used for binary classification problems, where the output variable has two possible classes or categories (e.g., 0 or 1, True or False).\n",
        "\n",
        "It predicts the probability of an instance belonging to one of the two classes using a sigmoid function to map the output to a probability between 0 and 1.\n",
        "\n",
        "**Softmax Regression (Multiclass Classification):**\n",
        "\n",
        "Softmax regression (also known as multinomial logistic regression) is used for multiclass classification problems, where the output variable has more than two possible classes (e.g., class 0, class 1, class 2).\n",
        "\n",
        "It uses the softmax function, which outputs a probability distribution over multiple classes, ensuring that the probabilities sum to 1.\n",
        "\n",
        "**2. Output Function:**\n",
        "\n",
        "**Logistic Regression:**\n",
        "\n",
        "Uses the sigmoid function to map the input to a probability between 0 and 1:\n",
        "P(y=1∣X)= 1/ 1+e to the power−(w Tx+b)\n",
        "\n",
        "This gives the probability of the instance belonging to the positive class (class 1), and the probability of class 0 is simply 1−P(y=1).\n",
        "\n",
        "**Softmax Regression:**\n",
        "\n",
        "Uses the softmax function to convert logits into probabilities for multiple classes:\n",
        "P(y=k∣X)= e w kT x+b k/∑ j=1C e w jTx+b j\n",
        "\n",
        "Here,\n",
        "C is the number of classes, and the softmax function gives the probability distribution over all classes (i.e., the sum of all probabilities equals 1).\n",
        "\n",
        "**3. Number of Outputs:**\n",
        "\n",
        "**Logistic Regression:**\n",
        "\n",
        "The output of logistic regression is a single probability value, indicating the likelihood of a class (usually class 1) versus the other (class 0).\n",
        "\n",
        "**Softmax Regression:**\n",
        "\n",
        "The output of softmax regression is a vector of probabilities, each corresponding to one of the multiple classes in a multiclass problem.\n",
        "\n",
        "**4. Use Cases:**\n",
        "\n",
        "**Logistic Regression:**\n",
        "\n",
        "Best used for binary classification problems (e.g., predicting whether an email is spam or not).\n",
        "\n",
        "**Softmax Regression:**\n",
        "\n",
        "Best used for multiclass classification problems (e.g., classifying an image as either \"cat\", \"dog\", or \"bird\").\n",
        "\n",
        "**Q19:-How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?**\n",
        "\n",
        "**Ans:**\n",
        "When choosing between One-vs-Rest (OvR) and Softmax for multiclass classification, it's important to understand the key differences between these two approaches and consider the specific characteristics of your problem. Here's a breakdown of each method and guidance on when to choose one over the other.\n",
        "\n",
        "**1. One-vs-Rest (OvR) Classification**\n",
        "\n",
        "**Overview:**\n",
        "\n",
        "One-vs-Rest (OvR), also known as One-vs-All, is a strategy for multiclass classification where a separate binary classifier is trained for each class.\n",
        "\n",
        "For each classifier, the model predicts whether an instance belongs to that class (positive class) or to any other class (negative class).\n",
        "\n",
        "During prediction, the class corresponding to the classifier that gives the highest score is chosen as the predicted class.\n",
        "\n",
        "**2. Softmax (Multinomial Logistic Regression)**\n",
        "\n",
        "**Overview:**\n",
        "\n",
        "Softmax regression is a direct approach to multiclass classification. It generalizes logistic regression to multiple classes by using the softmax function to compute probabilities for each class, and the predicted class is the one with the highest probability.\n",
        "\n",
        "It is typically used when the target is a categorical variable with multiple classes.\n",
        "\n",
        "**When to Choose One-vs-Rest (OvR) vs. Softmax Regression**\n",
        "\n",
        "Here are some key considerations for choosing between OvR and Softmax:\n",
        "\n",
        "**Number of Classes:**\n",
        "\n",
        "OvR can handle a large number of classes efficiently because it trains a separate classifier for each class, while Softmax may be more computationally expensive as the number of classes increases (since the entire model needs to consider all classes together).\n",
        "\n",
        "If you have a very large number of classes, OvR may be a better choice.\n",
        "\n",
        "**Model Complexity and Interpretability:**\n",
        "\n",
        "Softmax provides a single, unified model and is typically easier to interpret when you want a direct model for multiclass classification.\n",
        "\n",
        "OvR can be less interpretable because you have multiple models, and each model only provides binary outputs.\n",
        "\n",
        "**Performance Considerations:**\n",
        "\n",
        "If the classes are not linearly separable or you suspect there may be some overlap between the classes, Softmax can provide a more coherent model since it handles all classes simultaneously and gives a probability distribution over all classes.\n",
        "\n",
        "OvR could work well when each class is distinct and binary classifiers trained independently work well.\n",
        "\n",
        "**Probabilistic Output:**\n",
        "\n",
        "Softmax ensures that the output probabilities across classes sum to 1, making it a better choice when you need a clear probabilistic interpretation (e.g., ranking probabilities).\n",
        "\n",
        "OvR doesn't guarantee that the output probabilities will sum to 1, which can make interpretation of the probabilities less meaningful.\n",
        "\n",
        "**Scalability for Large Datasets:**\n",
        "\n",
        "If you're dealing with a large dataset with many features and classes, Softmax could be a more efficient choice for training a single model.\n",
        "\n",
        "OvR requires training multiple binary classifiers, which can increase training time as the number of classes grows.\n",
        "\n",
        "**Q20:-How do we interpret coefficients in Logistic Regression?**\n",
        "\n",
        "**Ans:-**\n",
        "Interpreting the coefficients in Logistic Regression can help us understand the influence of each feature on the model’s predictions. The coefficients represent how the log-odds (the logarithm of the odds ratio) of the target class change with a one-unit increase in the corresponding feature. Here's a detailed guide on how to interpret them in Python:\n",
        "\n",
        "**1. Logistic Regression Model Overview**\n",
        "\n",
        "In a logistic regression model, we try to model the probability\n",
        "P(y=1∣X) using the logistic (sigmoid) function:\n",
        "\n",
        "P(y=1∣X)= 1/1+e to the power −(b0+b1X1+b2X2+⋯+bnXn)\n",
        "\n",
        "P(y=1∣X) is the probability that the target\n",
        "𝑦\n",
        "y is class 1, given the input features 𝑋.\n",
        "\n",
        "b0,b1​,…,bnare the coefficients (weights) of the model.\n",
        "\n",
        "X1,X2,…,Xn are the input features.\n",
        "\n",
        "**2. Interpretation of Coefficients**\n",
        "\n",
        "The coefficient bi represents the change in the log-odds of the target variable for a one-unit increase in the corresponding feature X i, while holding all other features constant.\n",
        "\n",
        "Positive coefficient: If bi>0, then an increase in 𝑋𝑖 increases the log-odds of the target being class 1, which increases the probability of class 1.\n",
        "\n",
        "Negative coefficient: If 𝑏𝑖<0, then an increase in  decreases the log-odds of the target being class 1, which decreases the probability of class 1.\n",
        "\n",
        "The magnitude of the coefficient tells you the strength of the relationship. A large positive or negative coefficient indicates a strong effect on the outcome.\n",
        "\n",
        "Odds Ratio: You can exponentiate the coefficient to interpret it in terms of odds ratio. The odds ratio represents the multiplicative change in the odds for a one-unit increase in the feature.\n",
        "\n",
        "Odds Ratio=𝑒𝑏𝑖\n",
        "\n",
        "odds ratio > 1 means that as the feature value increases, the odds of the target class occurring increase.\n",
        "\n",
        "An odds ratio < 1 means that as the feature value increases, the odds of the target class occurring decrease.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pBleqtlBpybn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s5D3SQ9PpilH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aecfe9a-8910-4795-ef33-270b0bbcab64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 100.00%\n"
          ]
        }
      ],
      "source": [
        "#Q1:-Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2:-Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "import warnings\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model with L1 regularization (Lasso)\n",
        "# Suppress ConvergenceWarning\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
        "    model = LogisticRegression(penalty='l1', solver='saga', max_iter=200)\n",
        "    model.fit(X_train, y_train) #training the model\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f\"Model Accuracy with L1 Regularization (Lasso): {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHA8saQKA-xP",
        "outputId": "8a968f57-29ee-4ad0-93f3-e963e4914cb8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L1 Regularization (Lasso): 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3:-Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model with L2 regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f\"Model Accuracy with L2 Regularization (Ridge): {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Print the model coefficients\n",
        "print(\"Model Coefficients (L2 Regularization):\")\n",
        "print(model.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJWiDiSDBxIG",
        "outputId": "9ee43f5f-a97b-4b5c-eb9a-b9084004e5ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L2 Regularization (Ridge): 100.00%\n",
            "Model Coefficients (L2 Regularization):\n",
            "[[-0.39340204  0.96258576 -2.37510761 -0.99874603]\n",
            " [ 0.50840364 -0.25486503 -0.21301366 -0.77575487]\n",
            " [-0.1150016  -0.70772072  2.58812127  1.77450091]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4:-Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet') .\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler  # To scale the features\n",
        "\n",
        "# Load dataset (Iris dataset for simplicity)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target variable\n",
        "\n",
        "# For simplicity, we will classify just two classes (class 0 and class 1)\n",
        "X = X[y != 2]  # Use only class 0 and class 1\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale features for better convergence of the logistic regression model\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
        "# l1_ratio controls the mix of L1 and L2 penalties (0 for L2, 1 for L1)\n",
        "logreg = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=10000)\n",
        "\n",
        "# Fit the model\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy of Logistic Regression with Elastic Net Regularization: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1QN8sQOhFPe",
        "outputId": "165c6f48-8c16-486a-f5c6-adc4ce28a684"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Logistic Regression with Elastic Net Regularization: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q5:-Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier  # For One-vs-Rest\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler  # To scale the features\n",
        "\n",
        "# Load the Iris dataset (multiclass classification)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target variable (3 classes)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale features to improve convergence of the model\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "logreg = LogisticRegression(solver='liblinear', max_iter=10000)\n",
        "\n",
        "# Wrap Logistic Regression with OneVsRestClassifier\n",
        "ovr = OneVsRestClassifier(logreg)\n",
        "\n",
        "# Train the model\n",
        "ovr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ovr.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy of Logistic Regression with OneVsRestClassifier: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tj5_NaqWhxxC",
        "outputId": "4ec6fda4-4916-4d1b-8eba-84d16b9db600"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Logistic Regression with OneVsRestClassifier: 0.9111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q6:-Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (for example, Iris dataset)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize a logistic regression model (set solver to liblinear for penalty compatibility)\n",
        "log_reg = LogisticRegression(max_iter=200, solver='liblinear')\n",
        "\n",
        "# Define hyperparameters grid to tune (C and penalty)\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
        "    'penalty': ['l1', 'l2']        # Regularization type\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV on the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters from the grid search\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Get the best estimator (Logistic Regression model)\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the best parameters and accuracy\n",
        "print(\"Best Parameters: \", best_params)\n",
        "print(\"Test Accuracy: {:.4f}\".format(accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEAtIYo3mGoP",
        "outputId": "2f8f8d3a-280d-4b2f-9f58-c89425125e27"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters:  {'C': 10, 'penalty': 'l2'}\n",
            "Test Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q7:-Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (for example, Iris dataset)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=200, solver='liblinear')\n",
        "\n",
        "# Initialize Stratified K-Fold cross-validation (5 splits)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# List to store accuracy for each fold\n",
        "accuracies = []\n",
        "\n",
        "# Perform Stratified K-Fold Cross-Validation\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    # Split data into train and test sets for this fold\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Train the model on the training set\n",
        "    log_reg.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = log_reg.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy for this fold\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Calculate the average accuracy across all folds\n",
        "average_accuracy = np.mean(accuracies)\n",
        "\n",
        "# Print the average accuracy\n",
        "print(f\"Average Accuracy: {average_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9YODzhWntkA",
        "outputId": "9a208231-63b9-4f88-b279-c635e1c78d25"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy: 0.9600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "KoBPlqdLpI8C",
        "outputId": "5aad4d2c-369c-4336-8d2a-a957d2cee8a0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4e048199-0e73-4040-aa2b-6da030e6d58d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4e048199-0e73-4040-aa2b-6da030e6d58d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving auto-mpg (1).csv to auto-mpg (1) (2).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q8:-Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the 'auto-mpg' dataset from CSV file\n",
        "# Replace 'auto-mpg.csv' with the actual path to your CSV file\n",
        "data = pd.read_csv('auto-mpg (1).csv')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(data.head())\n",
        "\n",
        "# Clean the data:\n",
        "\n",
        "data = data.replace('?', pd.NA)\n",
        "\n",
        "# 1. Drop rows with missing values (or handle them)\n",
        "data = data.dropna()\n",
        "\n",
        "# 2. For simplicity, let's classify the target 'mpg' into binary classes:\n",
        "# Let's assume cars with mpg > 20 are 'high mpg' (1), otherwise 'low mpg' (0)\n",
        "\n",
        "# Create a binary target variable based on 'mpg'\n",
        "data['mpg_class'] = data['mpg'].apply(lambda x: 1 if x > 20 else 0)\n",
        "\n",
        "# Features (X) - Exclude the target column and any non-numeric columns (like 'name')\n",
        "X = data.drop(['mpg', 'mpg_class', 'name'], axis=1)\n",
        "\n",
        "# Target (y) - The new binary target variable\n",
        "y = data['mpg_class']\n",
        "\n",
        "# Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=200, solver='liblinear')\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdTqE8nIqPJq",
        "outputId": "889e562f-56c8-4cc5-bcbd-bef490dfd79e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    mpg  cylinders  displacement horsepower  weight  acceleration  year  \\\n",
            "0  18.0          8         307.0        130    3504          12.0    70   \n",
            "1  15.0          8         350.0        165    3693          11.5    70   \n",
            "2  18.0          8         318.0        150    3436          11.0    70   \n",
            "3  16.0          8         304.0        150    3433          12.0    70   \n",
            "4  17.0          8         302.0        140    3449          10.5    70   \n",
            "\n",
            "   origin                       name  \n",
            "0       1  chevrolet chevelle malibu  \n",
            "1       1          buick skylark 320  \n",
            "2       1         plymouth satellite  \n",
            "3       1              amc rebel sst  \n",
            "4       1                ford torino  \n",
            "Accuracy: 0.8861\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q9:-Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "# Step 1: Load the Titanic dataset (replace with your file path if necessary)\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "\n",
        "# Handle missing data for 'Age' column (numerical)\n",
        "imputer_age = SimpleImputer(strategy='mean')  # Replace missing Age with the mean\n",
        "df['Age'] = imputer_age.fit_transform(df[['Age']])\n",
        "\n",
        "# Handle missing data for 'Embarked' column (categorical)\n",
        "imputer_embarked = SimpleImputer(strategy='most_frequent')  # Replace missing Embarked with the most frequent value\n",
        "# Change: Pass a Series (1-dimensional) to fit_transform\n",
        "df['Embarked'] = imputer_embarked.fit_transform(df['Embarked'].values.reshape(-1, 1))[:, 0]\n",
        "\n",
        "# Drop the 'Cabin' column due to too many missing values, and drop 'Name' and 'Ticket' as they are not needed\n",
        "df.drop(columns=['Cabin', 'Name', 'Ticket'], inplace=True)\n",
        "\n",
        "# Step 3: Convert categorical features to numerical values\n",
        "# Convert 'Sex' using LabelEncoder\n",
        "le_sex = LabelEncoder()\n",
        "df['Sex'] = le_sex.fit_transform(df['Sex'])\n",
        "\n",
        "# Convert 'Embarked' to numerical values using LabelEncoder\n",
        "le_embarked = LabelEncoder()\n",
        "df['Embarked'] = le_embarked.fit_transform(df['Embarked'])\n",
        "\n",
        "# Step 4: Define features (X) and target (y)\n",
        "X = df.drop(columns=['Survived'])  # Features (drop the target column)\n",
        "y = df['Survived']  # Target variable\n",
        "\n",
        "# Step 5: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Apply Standardization (Scaling) to the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 7: Train Logistic Regression model with a higher max_iter and solver 'saga'\n",
        "log_reg = LogisticRegression(max_iter=1000, solver='saga', C=1)  # Increase max_iter to 1000 and use 'saga' solver\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 8: Save the trained model using joblib\n",
        "joblib.dump(log_reg, 'logistic_regression_model.pkl')\n",
        "joblib.dump(scaler, 'scaler.pkl')  # Save the scaler as well for consistent preprocessing\n",
        "\n",
        "# Step 9: Load the trained model from file\n",
        "loaded_model = joblib.load('logistic_regression_model.pkl')\n",
        "loaded_scaler = joblib.load('scaler.pkl')\n",
        "\n",
        "# Step 10: Make predictions using the loaded model\n",
        "X_test_scaled_loaded = loaded_scaler.transform(X_test)  # Scale the test data with the loaded scaler\n",
        "y_pred = loaded_model.predict(X_test_scaled_loaded)\n",
        "\n",
        "# Step 11: Evaluate the accuracy on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 12: Print the result\n",
        "print(f\"Accuracy of the loaded model: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mV-DiiSsYHV",
        "outputId": "e5569bee-4285-4bfe-c0cd-6263ce5632f6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the loaded model: 0.8101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q10:-Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris  # Example dataset\n",
        "\n",
        "# Load an example dataset (you can replace this with your own dataset)\n",
        "data = load_iris()  # Here, we are using the Iris dataset for demonstration\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# For simplicity, we'll perform binary classification (classifying class 0 vs class 1)\n",
        "y_binary = (y == 0).astype(int)  # Class 0 vs class 1 binary classification\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Set up the parameter grid for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
        "    'penalty': ['l1', 'l2'],              # Type of regularization\n",
        "    'solver': ['liblinear', 'saga']       # Solvers to use\n",
        "}\n",
        "\n",
        "# Apply RandomizedSearchCV for hyperparameter tuning\n",
        "random_search = RandomizedSearchCV(log_reg, param_distributions=param_dist,\n",
        "                                   n_iter=24, cv=5, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Train the model using RandomizedSearchCV\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters found by RandomizedSearchCV\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred = random_search.best_estimator_.predict(X_test)\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy of the best model\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVvIvL4LwL9B",
        "outputId": "7f727f59-45ba-4a97-a356-7391bc033b01"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'solver': 'saga', 'penalty': 'l2', 'C': 0.01}\n",
            "Test Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q11:-Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Example: Load a dataset from sklearn (replace with your own dataset)\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset (binary classification problem: class 0 vs class 1)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Convert to binary classification problem (class 0 vs class 1)\n",
        "y_binary = (y == 0).astype(int)  # Convert classes into binary: class 0 vs class 1\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using a heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "MiEl3bEmxOZ_",
        "outputId": "44daa1e4-34af-4234-d6ac-51845755481e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q12:-Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Example: Load a dataset from sklearn (replace with your own dataset)\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset (binary classification problem: class 0 vs class 1)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Convert to binary classification problem (class 0 vs class 1)\n",
        "y_binary = (y == 0).astype(int)  # Convert classes into binary: class 0 vs class 1\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Calculate Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Also calculate accuracy for completeness\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT0uj8vzxsT3",
        "outputId": "e9eae341-5485-4d93-a8ef-54872537d873"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.0000\n",
            "Recall: 1.0000\n",
            "F1-Score: 1.0000\n",
            "Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q13:-Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Step 1: Create an imbalanced dataset (or you can load your own dataset)\n",
        "# Generate a binary classification dataset with imbalance\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10,\n",
        "                           n_classes=2, weights=[0.1, 0.9], flip_y=0, random_state=42)\n",
        "\n",
        "# Check class distribution to confirm imbalance\n",
        "print(f\"Class distribution in y: {np.bincount(y)}\")\n",
        "\n",
        "# Step 2: Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Initialize Logistic Regression model with class weights\n",
        "log_reg = LogisticRegression(class_weight='balanced', max_iter=200)\n",
        "\n",
        "# Step 4: Train the Logistic Regression model on the imbalanced dataset\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the model using Precision, Recall, F1-Score, and Accuracy\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 7: Print the evaluation metrics\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvxBmdvQyExo",
        "outputId": "48fd5839-2b16-40fc-879c-aa2d24c2ac2a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution in y: [100 900]\n",
            "Precision: 0.9866\n",
            "Recall: 0.8352\n",
            "F1-Score: 0.9046\n",
            "Accuracy: 0.8450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q14:-Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Step 1: Load the Titanic dataset (replace with your file path if necessary)\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "\n",
        "# Handle missing data for 'Age' column (numerical)\n",
        "imputer_age = SimpleImputer(strategy='mean')  # Replace missing Age with the mean\n",
        "df['Age'] = imputer_age.fit_transform(df[['Age']])\n",
        "\n",
        "# Handle missing data for 'Embarked' column (categorical)\n",
        "imputer_embarked = SimpleImputer(strategy='most_frequent')  # Replace missing Embarked with the most frequent value\n",
        "# Change: Pass a Series (1-dimensional) to fit_transform\n",
        "df['Embarked'] = imputer_embarked.fit_transform(df['Embarked'].values.reshape(-1, 1))[:, 0]\n",
        "\n",
        "# Drop the 'Cabin' column due to too many missing values, and drop 'Name' and 'Ticket' as they are not needed\n",
        "df.drop(columns=['Cabin', 'Name', 'Ticket'], inplace=True)\n",
        "\n",
        "# Step 3: Convert categorical features to numerical values\n",
        "# Convert 'Sex' using LabelEncoder\n",
        "le_sex = LabelEncoder()\n",
        "df['Sex'] = le_sex.fit_transform(df['Sex'])\n",
        "\n",
        "# Convert 'Embarked' to numerical values using LabelEncoder\n",
        "le_embarked = LabelEncoder()\n",
        "df['Embarked'] = le_embarked.fit_transform(df['Embarked'])\n",
        "\n",
        "# Step 4: Define features (X) and target (y)\n",
        "X = df.drop(columns=['Survived'])  # Features (drop the target column)\n",
        "y = df['Survived']  # Target variable\n",
        "\n",
        "# Step 5: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Initialize and train the Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=500)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Make predictions on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Step 8: Evaluate the model using Accuracy, Precision, Recall, and F1-Score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Step 9: Print the evaluation metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzv7gWDb0mUK",
        "outputId": "bf052ee2-32ab-49bf-ed8b-281b34849c05"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8101\n",
            "Precision: 0.7857\n",
            "Recall: 0.7432\n",
            "F1-Score: 0.7639\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q15:-Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scalin.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Step 1: Load the Titanic dataset (replace with your file path if necessary)\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "\n",
        "# Handle missing data for 'Age' column (numerical)\n",
        "imputer_age = SimpleImputer(strategy='mean')  # Replace missing Age with the mean\n",
        "df['Age'] = imputer_age.fit_transform(df[['Age']])\n",
        "\n",
        "# Handle missing data for 'Embarked' column (categorical)\n",
        "imputer_embarked = SimpleImputer(strategy='most_frequent')  # Replace missing Embarked with the most frequent value\n",
        "# Change: Pass a Series (1-dimensional) to fit_transform or Reshape the series to a 2D array\n",
        "df['Embarked'] = imputer_embarked.fit_transform(df['Embarked'].values.reshape(-1, 1))[:, 0] #Reshaping the series to a 2D array\n",
        "\n",
        "# Drop the 'Cabin' column due to too many missing values, and drop 'Name' and 'Ticket' as they are not needed\n",
        "df.drop(columns=['Cabin', 'Name', 'Ticket'], inplace=True)\n",
        "\n",
        "# Step 3: Convert categorical features to numerical values\n",
        "# Convert 'Sex' using LabelEncoder\n",
        "le_sex = LabelEncoder()\n",
        "df['Sex'] = le_sex.fit_transform(df['Sex'])\n",
        "\n",
        "# Convert 'Embarked' to numerical values using LabelEncoder\n",
        "le_embarked = LabelEncoder()\n",
        "df['Embarked'] = le_embarked.fit_transform(df['Embarked'])\n",
        "\n",
        "# Step 4: Define features (X) and target (y)\n",
        "X = df.drop(columns=['Survived'])  # Features (drop the target column)\n",
        "y = df['Survived']  # Target variable\n",
        "\n",
        "# Step 5: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Train Logistic Regression without scaling\n",
        "log_reg = LogisticRegression(max_iter=500)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Make predictions and evaluate accuracy without scaling\n",
        "y_pred_no_scaling = log_reg.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scaling:.4f}\")\n",
        "\n",
        "# Step 8: Apply feature scaling (Standardization) to the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 9: Train Logistic Regression with scaling\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 10: Make predictions and evaluate accuracy with scaling\n",
        "y_pred_with_scaling = log_reg.predict(X_test_scaled)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "print(f\"Accuracy with scaling: {accuracy_with_scaling:.4f}\")\n",
        "\n",
        "# Step 11: Compare the results\n",
        "print(f\"Accuracy improvement: {accuracy_with_scaling - accuracy_no_scaling:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0e_1wj32vag",
        "outputId": "7ef03c07-8e5a-4b29-f004-10182802b062"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.8101\n",
            "Accuracy with scaling: 0.8101\n",
            "Accuracy improvement: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q16:-Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load the Titanic dataset (replace with your file path if necessary)\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "\n",
        "# Handle missing data for 'Age' column (numerical)\n",
        "imputer_age = SimpleImputer(strategy='mean')  # Replace missing Age with the mean\n",
        "df['Age'] = imputer_age.fit_transform(df[['Age']])\n",
        "\n",
        "# Handle missing data for 'Embarked' column (categorical)\n",
        "imputer_embarked = SimpleImputer(strategy='most_frequent')  # Replace missing Embarked with the most frequent value\n",
        "# Change: Pass a Series (1-dimensional) to fit_transform or reshape the series to a 2D array\n",
        "df['Embarked'] = imputer_embarked.fit_transform(df['Embarked'].values.reshape(-1, 1))[:, 0] #Reshaping the series to a 2D array with one column and fit_transform returns a 2D array, then select the first column [:,0] to assign it back to the 'Embarked' column of the DataFrame.\n",
        "\n",
        "# Drop the 'Cabin' column due to too many missing values, and drop 'Name' and 'Ticket' as they are not needed\n",
        "df.drop(columns=['Cabin', 'Name', 'Ticket'], inplace=True)\n",
        "\n",
        "# Step 3: Convert categorical features to numerical values\n",
        "# Convert 'Sex' using LabelEncoder\n",
        "le_sex = LabelEncoder()\n",
        "df['Sex'] = le_sex.fit_transform(df['Sex'])\n",
        "\n",
        "# Convert 'Embarked' to numerical values using LabelEncoder\n",
        "le_embarked = LabelEncoder()\n",
        "df['Embarked'] = le_embarked.fit_transform(df['Embarked'])\n",
        "\n",
        "# Step 4: Define features (X) and target (y)\n",
        "X = df.drop(columns=['Survived'])  # Features (drop the target column)\n",
        "y = df['Survived']  # Target variable\n",
        "\n",
        "# Step 5: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Apply Standardization (Scaling) to the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 7: Train Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=500)\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 8: Make predictions (probabilities) on the test set\n",
        "y_pred_prob = log_reg.predict_proba(X_test_scaled)[:, 1]  # We need the probabilities for the positive class\n",
        "\n",
        "# Step 9: Evaluate the model using ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Step 10: Plot the ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.4f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "7IC_EhCQ3aby",
        "outputId": "7a0d1430-9740-4600-9ca3-56b02aad42eb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.8758\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q17:-Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load the Titanic dataset (replace with your file path if necessary)\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "\n",
        "# Handle missing data for 'Age' column (numerical)\n",
        "imputer_age = SimpleImputer(strategy='mean')  # Replace missing Age with the mean\n",
        "df['Age'] = imputer_age.fit_transform(df[['Age']])\n",
        "\n",
        "# Handle missing data for 'Embarked' column (categorical)\n",
        "imputer_embarked = SimpleImputer(strategy='most_frequent')  # Replace missing Embarked with the most frequent value\n",
        "# Change: Pass a Series (1-dimensional) to fit_transform or reshape the series to a 2D array\n",
        "df['Embarked'] = imputer_embarked.fit_transform(df['Embarked'].values.reshape(-1, 1))[:, 0] #Reshaping the series to a 2D array with one column and fit_transform returns a 2D array, then select the first column [:,0] to assign it back to the 'Embarked' column of the DataFrame.\n",
        "\n",
        "# Drop the 'Cabin' column due to too many missing values, and drop 'Name' and 'Ticket' as they are not needed\n",
        "df.drop(columns=['Cabin', 'Name', 'Ticket'], inplace=True)\n",
        "\n",
        "# Step 3: Convert categorical features to numerical values\n",
        "# Convert 'Sex' using LabelEncoder\n",
        "le_sex = LabelEncoder()\n",
        "df['Sex'] = le_sex.fit_transform(df['Sex'])\n",
        "\n",
        "# Convert 'Embarked' to numerical values using LabelEncoder\n",
        "le_embarked = LabelEncoder()\n",
        "df['Embarked'] = le_embarked.fit_transform(df['Embarked'])\n",
        "\n",
        "# Step 4: Define features (X) and target (y)\n",
        "X = df.drop(columns=['Survived'])  # Features (drop the target column)\n",
        "y = df['Survived']  # Target variable\n",
        "\n",
        "# Step 5: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Apply Standardization (Scaling) to the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 7: Train Logistic Regression model with custom learning rate (C=0.5)\n",
        "log_reg = LogisticRegression(C=0.5, max_iter=500)\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 8: Make predictions on the test set\n",
        "y_pred = log_reg.predict(X_test_scaled)\n",
        "\n",
        "# Step 9: Evaluate the model using accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with custom learning rate (C=0.5): {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYLwTzkB4TAj",
        "outputId": "a849dcdb-ba84-4547-97cc-e6b795c0c904"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with custom learning rate (C=0.5): 0.8101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q18:-Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load the Titanic dataset (replace with your file path if necessary)\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "\n",
        "# Handle missing data for 'Age' column (numerical)\n",
        "imputer_age = SimpleImputer(strategy='mean')  # Replace missing Age with the mean\n",
        "df['Age'] = imputer_age.fit_transform(df[['Age']])\n",
        "\n",
        "# Handle missing data for 'Embarked' column (categorical)\n",
        "imputer_embarked = SimpleImputer(strategy='most_frequent')  # Replace missing Embarked with the most frequent value\n",
        "# Change: Pass a Series (1-dimensional) to fit_transform or reshape the series to a 2D array\n",
        "df['Embarked'] = imputer_embarked.fit_transform(df['Embarked'].values.reshape(-1, 1))[:, 0] #Reshaping the series to a 2D array with one column and fit_transform returns a 2D array, then select the first column [:,0] to assign it back to the 'Embarked' column of the DataFrame.\n",
        "\n",
        "# Drop the 'Cabin' column due to too many missing values, and drop 'Name' and 'Ticket' as they are not needed\n",
        "df.drop(columns=['Cabin', 'Name', 'Ticket'], inplace=True)\n",
        "\n",
        "# Step 3: Convert categorical features to numerical values\n",
        "# Convert 'Sex' using LabelEncoder\n",
        "le_sex = LabelEncoder()\n",
        "df['Sex'] = le_sex.fit_transform(df['Sex'])\n",
        "\n",
        "# Convert 'Embarked' to numerical values using LabelEncoder\n",
        "le_embarked = LabelEncoder()\n",
        "df['Embarked'] = le_embarked.fit_transform(df['Embarked'])\n",
        "\n",
        "# Step 4: Define features (X) and target (y)\n",
        "X = df.drop(columns=['Survived'])  # Features (drop the target column)\n",
        "y = df['Survived']  # Target variable\n",
        "\n",
        "# Step 5: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Apply Standardization (Scaling) to the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 7: Train Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=500)\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 8: Evaluate the model using accuracy\n",
        "y_pred = log_reg.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Step 9: Identify important features based on model coefficients\n",
        "coefficients = log_reg.coef_[0]\n",
        "feature_names = X.columns\n",
        "\n",
        "# Create a DataFrame to display the feature importance (coefficients)\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': coefficients\n",
        "})\n",
        "\n",
        "# Sort the features by the absolute value of coefficients to find the most important features\n",
        "feature_importance['Absolute Coefficient'] = feature_importance['Coefficient'].abs()\n",
        "feature_importance = feature_importance.sort_values(by='Absolute Coefficient', ascending=False)\n",
        "\n",
        "# Display the sorted feature importance\n",
        "print(\"\\nImportant Features Based on Model Coefficients:\")\n",
        "print(feature_importance[['Feature', 'Coefficient']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2dsstbp5j2X",
        "outputId": "43a9270e-e9a9-455e-c932-2ae6f6be2332"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8101\n",
            "\n",
            "Important Features Based on Model Coefficients:\n",
            "       Feature  Coefficient\n",
            "2          Sex    -1.287758\n",
            "1       Pclass    -0.778807\n",
            "3          Age    -0.404354\n",
            "4        SibSp    -0.341565\n",
            "7     Embarked    -0.174754\n",
            "6         Fare     0.126322\n",
            "5        Parch    -0.109986\n",
            "0  PassengerId     0.093784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q19:-Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, cohen_kappa_score # Import cohen_kappa_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load the Titanic dataset (replace with your file path if necessary)\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "\n",
        "# Handle missing data for 'Age' column (numerical)\n",
        "imputer_age = SimpleImputer(strategy='mean')  # Replace missing Age with the mean\n",
        "df['Age'] = imputer_age.fit_transform(df[['Age']])\n",
        "\n",
        "# Handle missing data for 'Embarked' column (categorical)\n",
        "imputer_embarked = SimpleImputer(strategy='most_frequent')  # Replace missing Embarked with the most frequent value\n",
        "# Change: Pass a Series (1-dimensional) to fit_transform or reshape the series to a 2D array\n",
        "df['Embarked'] = imputer_embarked.fit_transform(df['Embarked'].values.reshape(-1, 1))[:, 0] #Reshaping the series to a 2D array with one column and fit_transform returns a 2D array, then select the first column [:,0] to assign it back to the 'Embarked' column of the DataFrame.\n",
        "\n",
        "# Drop the 'Cabin' column due to too many missing values, and drop 'Name' and 'Ticket' as they are not needed\n",
        "df.drop(columns=['Cabin', 'Name', 'Ticket'], inplace=True)\n",
        "\n",
        "# Step 3: Convert categorical features to numerical values\n",
        "# Convert 'Sex' using LabelEncoder\n",
        "le_sex = LabelEncoder()\n",
        "df['Sex'] = le_sex.fit_transform(df['Sex'])\n",
        "\n",
        "# Convert 'Embarked' to numerical values using LabelEncoder\n",
        "le_embarked = LabelEncoder()\n",
        "df['Embarked'] = le_embarked.fit_transform(df['Embarked'])\n",
        "\n",
        "# Step 4: Define features (X) and target (y)\n",
        "X = df.drop(columns=['Survived'])  # Features (drop the target column)\n",
        "y = df['Survived']  # Target variable\n",
        "\n",
        "# Step 5: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Apply Standardization (Scaling) to the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 7: Train Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=500)\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 8: Make predictions on the test set\n",
        "y_pred = log_reg.predict(X_test_scaled)\n",
        "\n",
        "# Step 9: Evaluate the model using Cohen's Kappa Score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohen's Kappa Score: {kappa_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRxBHzmt5_-S",
        "outputId": "85a98581-a257-43e5-8d7c-82abd10bbfaf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.6052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q20:-Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, cohen_kappa_score, precision_recall_curve # Import precision_recall_curve\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load the Titanic dataset (replace with your file path if necessary)\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "\n",
        "# Handle missing data for 'Age' column (numerical)\n",
        "imputer_age = SimpleImputer(strategy='mean')  # Replace missing Age with the mean\n",
        "df['Age'] = imputer_age.fit_transform(df[['Age']])\n",
        "\n",
        "# Handle missing data for 'Embarked' column (categorical)\n",
        "imputer_embarked = SimpleImputer(strategy='most_frequent')  # Replace missing Embarked with the most frequent value\n",
        "# Change: Pass a Series (1-dimensional) to fit_transform or reshape the series to a 2D array\n",
        "df['Embarked'] = imputer_embarked.fit_transform(df['Embarked'].values.reshape(-1, 1))[:, 0] #Reshaping the series to a 2D array with one column and fit_transform returns a 2D array, then select the first column [:,0] to assign it back to the 'Embarked' column of the DataFrame.\n",
        "\n",
        "# Drop the 'Cabin' column due to too many missing values, and drop 'Name' and 'Ticket' as they are not needed\n",
        "df.drop(columns=['Cabin', 'Name', 'Ticket'], inplace=True)\n",
        "\n",
        "# Step 3: Convert categorical features to numerical values\n",
        "# Convert 'Sex' using LabelEncoder\n",
        "le_sex = LabelEncoder()\n",
        "df['Sex'] = le_sex.fit_transform(df['Sex'])\n",
        "\n",
        "# Convert 'Embarked' to numerical values using LabelEncoder\n",
        "le_embarked = LabelEncoder()\n",
        "df['Embarked'] = le_embarked.fit_transform(df['Embarked'])\n",
        "\n",
        "# Step 4: Define features (X) and target (y)\n",
        "X = df.drop(columns=['Survived'])  # Features (drop the target column)\n",
        "y = df['Survived']  # Target variable\n",
        "\n",
        "# Step 5: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Apply Standardization (Scaling) to the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 7: Train Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=500)\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 8: Predict probabilities for the positive class\n",
        "y_probs = log_reg.predict_proba(X_test_scaled)[:, 1]  # Get probabilities for the positive class\n",
        "\n",
        "# Step 9: Compute precision, recall, and thresholds\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_probs)\n",
        "\n",
        "# Step 10: Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='blue', label='Precision-Recall curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for Logistic Regression')\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "VgqR_IKx6aic",
        "outputId": "cc437ec3-8548-4a2a-9e7a-30cc53058369"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q21:-Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Load the Titanic dataset (replace with your file path if necessary)\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "\n",
        "# Handle missing data for 'Age' column (numerical)\n",
        "imputer_age = SimpleImputer(strategy='mean')  # Replace missing Age with the mean\n",
        "df['Age'] = imputer_age.fit_transform(df[['Age']])\n",
        "\n",
        "# Handle missing data for 'Embarked' column (categorical)\n",
        "imputer_embarked = SimpleImputer(strategy='most_frequent')  # Replace missing Embarked with the most frequent value\n",
        "# Change: Pass a Series (1-dimensional) to fit_transform or reshape the series to a 2D array\n",
        "df['Embarked'] = imputer_embarked.fit_transform(df['Embarked'].values.reshape(-1, 1))[:, 0] #Reshaping the series to a 2D array with one column and fit_transform returns a 2D array, then select the first column [:,0] to assign it back to the 'Embarked' column of the DataFrame.\n",
        "\n",
        "# Drop the 'Cabin' column due to too many missing values, and drop 'Name' and 'Ticket' as they are not needed\n",
        "df.drop(columns=['Cabin', 'Name', 'Ticket'], inplace=True)\n",
        "\n",
        "#Step 3: Convert categorical features to numerical values\n",
        "# Convert 'Sex' using LabelEncoder\n",
        "le_sex = LabelEncoder()\n",
        "df['Sex'] = le_sex.fit_transform(df['Sex'])\n",
        "\n",
        "# Convert 'Embarked' to numerical values using LabelEncoder\n",
        "le_embarked = LabelEncoder()\n",
        "df['Embarked'] = le_embarked.fit_transform(df['Embarked'])\n",
        "\n",
        "# Step 4: Define features (X) and target (y)\n",
        "X = df.drop(columns=['Survived'])  # Features (drop the target column)\n",
        "y = df['Survived']  # Target variable\n",
        "\n",
        "# Step 5: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Apply Standardization (Scaling) to the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 7: Train Logistic Regression models with different solvers and evaluate accuracy\n",
        "\n",
        "# Define solvers to test\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "accuracies = {}\n",
        "\n",
        "for solver in solvers:\n",
        "    # Train the model with the current solver\n",
        "    model = LogisticRegression(solver=solver, max_iter=500)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies[solver] = accuracy\n",
        "\n",
        "# Step 8: Print the accuracies for each solver\n",
        "for solver, accuracy in accuracies.items():\n",
        "    print(f\"Accuracy using solver '{solver}': {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QIa-XbR7Iu5",
        "outputId": "44ea6409-628a-4b9c-b8a6-e4f972400f2c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using solver 'liblinear': 0.8101\n",
            "Accuracy using solver 'saga': 0.8101\n",
            "Accuracy using solver 'lbfgs': 0.8101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q22:-Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Load the Titanic dataset (replace with your file path if necessary)\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "\n",
        "# Handle missing data for 'Age' column (numerical)\n",
        "imputer_age = SimpleImputer(strategy='mean')  # Replace missing Age with the mean\n",
        "df['Age'] = imputer_age.fit_transform(df[['Age']])\n",
        "\n",
        "# Handle missing data for 'Embarked' column (categorical)\n",
        "imputer_embarked = SimpleImputer(strategy='most_frequent')  # Replace missing Embarked with the most frequent value\n",
        "# Change: Pass a Series (1-dimensional) to fit_transform or reshape the series to a 2D array\n",
        "df['Embarked'] = imputer_embarked.fit_transform(df['Embarked'].values.reshape(-1, 1))[:, 0] #Reshaping the series to a 2D array with one column and fit_transform returns a 2D array, then select the first column [:,0] to assign it back to the 'Embarked' column of the DataFrame.\n",
        "\n",
        "# Drop the 'Cabin' column due to too many missing values, and drop 'Name' and 'Ticket' as they are not needed\n",
        "df.drop(columns=['Cabin', 'Name', 'Ticket'], inplace=True)\n",
        "\n",
        "# Step 3: Convert categorical features to numerical values\n",
        "# Convert 'Sex' using LabelEncoder\n",
        "le_sex = LabelEncoder()\n",
        "df['Sex'] = le_sex.fit_transform(df['Sex'])\n",
        "\n",
        "# Convert 'Embarked' to numerical values using LabelEncoder\n",
        "le_embarked = LabelEncoder()\n",
        "df['Embarked'] = le_embarked.fit_transform(df['Embarked'])\n",
        "\n",
        "# Step 4: Define features (X) and target (y)\n",
        "X = df.drop(columns=['Survived'])  # Features (drop the target column)\n",
        "y = df['Survived']  # Target variable\n",
        "\n",
        "# Step 5: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Apply Standardization (Scaling) to the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 7: Train Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=500)\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 8: Predict on the test set\n",
        "y_pred = log_reg.predict(X_test_scaled)\n",
        "\n",
        "# Step 9: Calculate the Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "# Step 10: Print the MCC score\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gvzecdp8G97",
        "outputId": "7098c435-c0eb-4594-b626-91a91fc56776"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.6059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q23:-Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Load the Titanic dataset (replace with your file path if necessary)\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "# Handle missing data for 'Age' column (numerical)\n",
        "imputer_age = SimpleImputer(strategy='mean')  # Replace missing Age with the mean\n",
        "df['Age'] = imputer_age.fit_transform(df[['Age']])\n",
        "\n",
        "# Handle missing data for 'Embarked' column (categorical)\n",
        "imputer_embarked = SimpleImputer(strategy='most_frequent')  # Replace missing Embarked with the most frequent value\n",
        "# Change: Pass a Series (1-dimensional) to fit_transform or reshape the series to a 2D array\n",
        "df['Embarked'] = imputer_embarked.fit_transform(df['Embarked'].values.reshape(-1, 1))[:, 0] #Reshaping the series to a 2D array with one column and fit_transform returns a 2D array, then select the first column [:,0] to assign it back to the 'Embarked' column of the DataFrame.\n",
        "\n",
        "# Drop the 'Cabin' column due to too many missing values, and drop 'Name' and 'Ticket' as they are not needed\n",
        "df.drop(columns=['Cabin', 'Name', 'Ticket'], inplace=True)\n",
        "\n",
        "# Step 3: Convert categorical features to numerical values\n",
        "# Convert 'Sex' using LabelEncoder\n",
        "le_sex = LabelEncoder()\n",
        "df['Sex'] = le_sex.fit_transform(df['Sex'])\n",
        "\n",
        "# Convert 'Embarked' to numerical values using LabelEncoder\n",
        "le_embarked = LabelEncoder()\n",
        "df['Embarked'] = le_embarked.fit_transform(df['Embarked'])\n",
        "\n",
        "# Step 4: Define features (X) and target (y)\n",
        "X = df.drop(columns=['Survived'])  # Features (drop the target column)\n",
        "y = df['Survived']  # Target variable\n",
        "\n",
        "# Step 5: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Train Logistic Regression on raw data\n",
        "log_reg_raw = LogisticRegression(max_iter=500)\n",
        "log_reg_raw.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Predict on the test set (raw data)\n",
        "y_pred_raw = log_reg_raw.predict(X_test)\n",
        "\n",
        "# Step 8: Calculate accuracy on raw data\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Step 9: Apply Standardization (Scaling) to the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 10: Train Logistic Regression on standardized data\n",
        "log_reg_scaled = LogisticRegression(max_iter=500)\n",
        "log_reg_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 11: Predict on the test set (standardized data)\n",
        "y_pred_scaled = log_reg_scaled.predict(X_test_scaled)\n",
        "\n",
        "# Step 12: Calculate accuracy on standardized data\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Step 13: Print the accuracy comparison\n",
        "print(f\"Accuracy on raw data: {accuracy_raw:.4f}\")\n",
        "print(f\"Accuracy on standardized data: {accuracy_scaled:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSxmf-HM8qmO",
        "outputId": "1c728bde-0d0d-4ea9-e1de-85860c987a6b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on raw data: 0.8101\n",
            "Accuracy on standardized data: 0.8101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q24:-Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Load the Titanic dataset (replace with your file path if necessary)\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "# Handle missing data for 'Age' column (numerical)\n",
        "imputer_age = SimpleImputer(strategy='mean')  # Replace missing Age with the mean\n",
        "df['Age'] = imputer_age.fit_transform(df[['Age']])\n",
        "\n",
        "# Handle missing data for 'Embarked' column (categorical)\n",
        "imputer_embarked = SimpleImputer(strategy='most_frequent')  # Replace missing Embarked with the most frequent value\n",
        "# Change: Pass a Series (1-dimensional) to fit_transform or reshape the series to a 2D array\n",
        "df['Embarked'] = imputer_embarked.fit_transform(df['Embarked'].values.reshape(-1, 1))[:, 0] #Reshaping the series to a 2D array with one column and fit_transform returns a 2D array, then select the first column [:,0] to assign it back to the 'Embarked' column of the DataFrame.\n",
        "\n",
        "# Drop the 'Cabin' column due to too many missing values, and drop 'Name' and 'Ticket' as they are not needed\n",
        "df.drop(columns=['Cabin', 'Name', 'Ticket'], inplace=True)\n",
        "\n",
        "# Step 3: Convert categorical features to numerical values\n",
        "# Convert 'Sex' using LabelEncoder\n",
        "le_sex = LabelEncoder()\n",
        "df['Sex'] = le_sex.fit_transform(df['Sex'])\n",
        "\n",
        "# Convert 'Embarked' to numerical values using LabelEncoder\n",
        "le_embarked = LabelEncoder()\n",
        "df['Embarked'] = le_embarked.fit_transform(df['Embarked'])\n",
        "\n",
        "# Step 4: Define features (X) and target (y)\n",
        "X = df.drop(columns=['Survived'])  # Features (drop the target column)\n",
        "y = df['Survived']  # Target variable\n",
        "\n",
        "# Step 5: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Apply Standardization (Scaling) to the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 7: Set up the parameter grid for 'C' (regularization strength)\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100]  # Range of C values to try\n",
        "}\n",
        "\n",
        "# Step 8: Initialize the Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=500)\n",
        "\n",
        "# Step 9: Set up GridSearchCV for cross-validation\n",
        "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Step 10: Train the model with cross-validation\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 11: Get the best C and corresponding score\n",
        "best_C = grid_search.best_params_['C']\n",
        "best_accuracy = grid_search.best_score_\n",
        "\n",
        "# Step 12: Evaluate the model on the test set using the best C\n",
        "best_log_reg = grid_search.best_estimator_\n",
        "y_pred = best_log_reg.predict(X_test_scaled)\n",
        "\n",
        "# Step 13: Calculate accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 14: Print the results\n",
        "print(f\"Best regularization strength (C): {best_C}\")\n",
        "print(f\"Best cross-validation accuracy: {best_accuracy:.4f}\")\n",
        "print(f\"Test set accuracy using the best model: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0Es_Jbr9Ofc",
        "outputId": "c6b72887-a2af-42bc-ddfd-4789ed5820c9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best regularization strength (C): 0.1\n",
            "Best cross-validation accuracy: 0.8005\n",
            "Test set accuracy using the best model: 0.7989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q25:-Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "# Step 1: Load the Titanic dataset (replace with your file path if necessary)\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "# Handle missing data for 'Age' column (numerical)\n",
        "imputer_age = SimpleImputer(strategy='mean')  # Replace missing Age with the mean\n",
        "df['Age'] = imputer_age.fit_transform(df[['Age']])\n",
        "\n",
        "# Handle missing data for 'Embarked' column (categorical)\n",
        "imputer_embarked = SimpleImputer(strategy='most_frequent')  # Replace missing Embarked with the most frequent value\n",
        "# Change: Pass a Series (1-dimensional) to fit_transform or reshape the series to a 2D array\n",
        "df['Embarked'] = imputer_embarked.fit_transform(df['Embarked'].values.reshape(-1, 1))[:, 0] #Reshaping the series to a 2D array with one column and fit_transform returns a 2D array, then select the first column [:,0] to assign it back to the 'Embarked' column of the DataFrame.\n",
        "\n",
        "# Drop the 'Cabin' column due to too many missing values, and drop 'Name' and 'Ticket' as they are not needed\n",
        "df.drop(columns=['Cabin', 'Name', 'Ticket'], inplace=True)\n",
        "\n",
        "# Step 3: Convert categorical features to numerical values\n",
        "# Convert 'Sex' using LabelEncoder\n",
        "le_sex = LabelEncoder()\n",
        "df['Sex'] = le_sex.fit_transform(df['Sex'])\n",
        "\n",
        "# Convert 'Embarked' to numerical values using LabelEncoder\n",
        "le_embarked = LabelEncoder()\n",
        "df['Embarked'] = le_embarked.fit_transform(df['Embarked'])\n",
        "\n",
        "# Step 4: Define features (X) and target (y)\n",
        "X = df.drop(columns=['Survived'])  # Features (drop the target column)\n",
        "y = df['Survived']  # Target variable\n",
        "\n",
        "# Step 5: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Apply Standardization (Scaling) to the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 7: Train Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=500)\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 8: Save the trained model using joblib\n",
        "joblib.dump(log_reg, 'logistic_regression_model.pkl')\n",
        "joblib.dump(scaler, 'scaler.pkl')  # Save the scaler as well for consistent preprocessing\n",
        "\n",
        "# Step 9: Load the trained model from file\n",
        "loaded_model = joblib.load('logistic_regression_model.pkl')\n",
        "loaded_scaler = joblib.load('scaler.pkl')\n",
        "\n",
        "# Step 10: Make predictions using the loaded model\n",
        "X_test_scaled_loaded = loaded_scaler.transform(X_test)  # Scale the test data with the loaded scaler\n",
        "y_pred = loaded_model.predict(X_test_scaled_loaded)\n",
        "\n",
        "# Step 11: Evaluate the accuracy on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 12: Print the result\n",
        "print(f\"Accuracy of the loaded model: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjYwYiCm-3HE",
        "outputId": "43394ab6-9638-42ff-a021-edb105a93144"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the loaded model: 0.8101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VfLt8kqt_k3O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}